# -*- coding: utf-8 -*-
"""Transfer Learning Fine Tuning in DL - BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MnaeBM_L-7kxFGirmu0U9bHi-Cou6Rmp
"""

!pip install transformers datasets

from transformers import BertTokenizer
from datasets import load_dataset
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
from transformers import TrainingArguments, Trainer, BertForSequenceClassification
from transformers import BertModel

# Load dataset (you can replace this with your CSV)
dataset = load_dataset("imdb",split='train[:2000]')
dataset = dataset.train_test_split(test_size=0.2)
train_texts = dataset['train']['text']
train_labels = dataset['train']['label']
val_texts = dataset['test']['text']
val_labels = dataset['test']['label']

#Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def tokenize(batch):
    return tokenizer(batch['text'],padding=True, truncation = True, return_tensors='pt')

dataset = dataset.map(lambda x: tokenizer(x['text'], padding="max_length", truncation=True), batched=True)
dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

#Load pre-trained model with new output head
model = BertForSequenceClassification.from_pretrained("bert-base-uncased",num_labels=2)

for param in model.bert.parameters():
    #param.requires_grad = False
    print(len(param))

total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params}")

trainable_params = sum(p.numel() for p in model.parameters () if p.requires_grad)
print(f"Trainable parameters : {trainable_params}")

for name,param in model.named_parameters():
  print(f"{name}:50 | shape: {tuple(param.shape)} | Trainable: {param.requires_grad}")

print(model.bert.embeddings)
print(model.bert.encoder.layer[11])
print(model.classifier)

print(model) #full model architecture, all layers

print(model.config) #Model config: hidden size, num labels, dropout, etc.

# Training args
from transformers import TrainingArguments, BertModel,TrainingArguments, BertConfig, PreTrainedModel, Trainer, BertTokenizer, BertForSequenceClassification

training_args = TrainingArguments(
    output_dir="./bert_output_1",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    logging_dir="./logs",
     report_to="none"
)

import os
os.environ["WANDB_DISABLED"] = "true"

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
)

from transformers import BertModel, BertTokenizer, PreTrainedModel, Trainer, TrainingArguments, BertConfig
from transformers.modeling_outputs import SequenceClassifierOutput

class BertBinaryClassifier(PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.classifier = nn.Linear(config.hidden_size, 2)  # Binary classification logits

        # Freeze all layers
        for param in self.bert.parameters():
            param.requires_grad = False

        # Unfreeze last 2 layers
        for layer in self.bert.encoder.layer[-2:]:
            for param in layer.parameters():
                param.requires_grad = True

        # Unfreeze pooler
        for param in self.bert.pooler.parameters():
            param.requires_grad = True

        # Classifier should be trainable
        for param in self.classifier.parameters():
            param.requires_grad = True

    def forward(self, input_ids=None, attention_mask=None, labels=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        logits = self.classifier(pooled_output)

        loss = None
        if labels is not None:
            loss_fn = nn.CrossEntropyLoss()
            loss = loss_fn(logits, labels)

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits
        )

config = BertConfig.from_pretrained("bert-base-uncased", num_labels=2)
model = BertBinaryClassifier(config)

training_args = TrainingArguments(
    output_dir="./bert_last2layers_finetune",
    num_train_epochs=8,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    save_strategy="no",
    report_to="none",  # Disable wandb etc.
    logging_dir="./logs"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
)

trainer.train()

def predict_text(text, model, tokenizer):
    # Preprocess input
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=128)

    # Put model in eval mode
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        predicted_class = torch.argmax(logits, dim=1).item()
        confidence = torch.softmax(logits, dim=1)[0][predicted_class].item()

    label_name = "Positive ✅" if predicted_class == 1 else "Negative ❌"
    print(f"Text: {text}")
    print(f"Prediction: {label_name} (Confidence: {confidence:.2f})")
    return predicted_class, confidence

